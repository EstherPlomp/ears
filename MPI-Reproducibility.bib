Automatically generated by Mendeley Desktop 1.17.13
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{steegen2016multiverse,
abstract = {Empirical research inevitably includes constructing a dataset by processing raw data into a form ready for statistical analysis. Data processing often involves choices among several reasonable options for excluding, transforming and coding data. We suggest that instead of performing only one analysis, researchers could perform a multiverse analysis, which involves performing all analyses across the whole set of alternatively processed data sets corresponding to a large set of reasonable scenarios. Using a worked example focusing on the effect of fertility on religiosity and political attitudes, we show that analyzing a single data set can be misleading, and propose a multiverse analysis as an alternative practice. A multiverse analysis offers an idea of how much the conclusions change because of arbitrary choices in data construction, and gives pointers as to which choices are most consequential in the fragility of the result.},
annote = {a multiverse analysis

performing all analyses across the whole set of alternatively processed data sets corresponding to
a large set of reasonable scenarios.


raw data do not uniquely give rise to a single data set for
analysis but rather to multiple alternatively processed
data sets, depending on the specific combination of
choices—a many worlds or multiverse of data sets.


multiverse analysis:

enhances transparency by providing a detailed picture of the robustness or fragility of statistical results, and it helps
identifying the key choices that conclusions hinge on


involves listing the different reasonable choices during
each step of data processing.

the analysis of interest
(in this case, an ANOVA or a logistic regression) is per-formed across all the alternatively constructed data sets.

alternative options for data construction
requires judgment about which options can be consid-ered reasonable and will typically depend on the experi-mental design, the research question, and the researchers
performing the research.


multiverse analysis is
valuable, regardless of the inferential framework (fre-quentist or Bayesian), and regardless of the specific way
uncertainty is quantified: a p value, an effect size, a con-fidence (Cumming, 2013) or credibility (Kruschke, 2010)
interval, or a Bayes factor (Morey {\&} Rouder, 2011).

MODEL MULTIVERSE

If the choice for
a single model specification out of the model multiverse
cannot be justified, a model multiverse analysis can be
performed to reveal the effect of this arbitrary choice on
the statistical result.

Patel, Burford, and Ioannidis (2015), focusing
on the choices in deciding which predictors and covari-ates to include. Such a model multiverse analysis is
related to perturbation analysis (Geisser, 1993) and to
sensitivity analysis in economics (e.g., Leamer, 1985) and
in Bayesian statistics (e.g., Kass {\&} Raftery, 1995),

more complete analysis, the multiverse of data sets
could be crossed with the multiverse of models to further
reveal the multiverse of statistical results.},
author = {Steegen, Sara and Tuerlinckx, Francis and Gelman, Andrew and Vanpaemel, Wolf},
doi = {10.1177/1745691616658637},
file = {:Users/rianaminocher/Documents/reading/Steegen{\_}et{\_}al{\_}2016{\_}Multiverse analysis.pdf:pdf},
isbn = {1745-6924 (Electronic)$\backslash$r1745-6916 (Linking)},
issn = {17456924},
journal = {Perspectives on Psychological Science},
keywords = {arbitrary choices,data processing,good research practices,multiverse analysis,selective reporting,transparency},
number = {5},
pages = {702--712},
pmid = {27694465},
title = {{Increasing Transparency Through a Multiverse Analysis}},
volume = {11},
year = {2016}
}
@article{wicherts2011willingness,
abstract = {BACKGROUND: The widespread reluctance to share published research data is often hypothesized to be due to the authors' fear that reanalysis may expose errors in their work or may produce conclusions that contradict their own. However, these hypotheses have not previously been studied systematically.$\backslash$n$\backslash$nMETHODS AND FINDINGS: We related the reluctance to share research data for reanalysis to 1148 statistically significant results reported in 49 papers published in two major psychology journals. We found the reluctance to share data to be associated with weaker evidence (against the null hypothesis of no effect) and a higher prevalence of apparent errors in the reporting of statistical results. The unwillingness to share data was particularly clear when reporting errors had a bearing on statistical significance.$\backslash$n$\backslash$nCONCLUSIONS: Our findings on the basis of psychological papers suggest that statistical results are particularly hard to verify when reanalysis is more likely to lead to contrasting conclusions. This highlights the importance of establishing mandatory data archiving policies.},
author = {Wicherts, Jelte M. and Bakker, Marjan and Molenaar, Dylan},
doi = {10.1371/journal.pone.0026828},
file = {:Users/rianaminocher/Documents/reading/Wicherts{\_}et{\_}al{\_}2011{\_}Willingness to share data.PDF:PDF},
isbn = {1932-6203 (Electronic)$\backslash$n1932-6203 (Linking)},
issn = {19326203},
journal = {PLoS ONE},
number = {11},
pages = {1--7},
pmid = {22073203},
title = {{Willingness to share research data is related to the strength of the evidence and the quality of reporting of statistical results}},
volume = {6},
year = {2011}
}
@article{yang2015value,
abstract = {The study protocol, publications, full study report detailing all analyses, and participant-level dataset constitute the main documentation of methods and results for health research. However, journal publications are available for only half of all studies and are plagued by selective reporting of methods and results. The protocol, full study report, and participant-level dataset are rarely available. The quality of information provided in study protocols and reports is variable and often incomplete. Inaccessibility of full information for the vast majority of studies wastes billions of dollars, introduces bias, and has a detrimental impact on patient care and research. To help improve this situation at a systemic level, three main actions are warranted. Firstly, it is importantthat academic institutions and funders reward investigators who fully disseminate their research protocols, reports, and participant-level datasets. Secondly, standards for the content of protocols, full study reports, and data sharing practices should be rigorously developed and adopted for all types of health research. Finally, journals, funders, sponsors, research ethics committees, regulators, and legislators should implement and enforce policies supporting study registration and availability of journal publications, full study reports, and participant-level datasets.},
archivePrefix = {arXiv},
arxivId = {15334406},
author = {Chan, An-Wen and Song, Fujian and Vickers, Andrew and Jefferson, Tom and Dickersin, Kay and G{\o}tzsche, Peter C. and Krumholz, Harlan M. and Ghersi, Davina and van der Worp, H. Bart},
doi = {10.1126/science.1249098.Sleep},
eprint = {15334406},
file = {:Users/rianaminocher/Documents/reading/Chan{\_}et{\_}al{\_}2014{\_}Reducing research waste.pdf:pdf},
isbn = {0000000000000},
issn = {1527-5418},
journal = {The Lancet},
number = {9913},
pages = {257--266},
pmid = {24655651},
title = {{Increasing value and reducing waste: addressing inaccessible research}},
volume = {383},
year = {2014}
}
@article{stodden2018empirical,
author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
doi = {10.1073/pnas.1708290115},
file = {:Users/rianaminocher/Documents/reading/Stodden{\_}et{\_}al{\_}2018{\_}Empirical computational reproducibility.pdf:pdf},
issn = {0027-8424},
journal = {Proceedings of the National Academy of Sciences},
number = {11},
pages = {2584--2589},
pmid = {29531050},
title = {{An empirical analysis of journal policy effectiveness for computational reproducibility}},
url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708290115},
volume = {115},
year = {2018}
}
@article{lowndes2017open,
abstract = {Reproducibility starts with having a transparent and streamlined workflow. Here, the authors describe how they achieved this using open data tools for the collaborative Ocean Health Index project.},
author = {Lowndes, Julia S.Stewart and Best, Benjamin D. and Scarborough, Courtney and Afflerbach, Jamie C. and Frazier, Melanie R. and O'Hara, Casey C. and Jiang, Ning and Halpern, Benjamin S.},
doi = {10.1038/s41559-017-0160},
file = {:Users/rianaminocher/Documents/reading/Lowndes{\_}et{\_}al{\_}2017{\_}Path to better science OHI.pdf:pdf},
isbn = {4690786631},
issn = {2397334X},
journal = {Nature Ecology and Evolution},
number = {6},
pmid = {28812630},
title = {{Our path to better science in less time using open data science tools}},
volume = {1},
year = {2017}
}
@article{gertler2018norm,
abstract = {Replication is essential for building confidence in research studies1, yet it is still the exception rather than the rule2,3. That is not necessarily because funding is unavailable — it is because the current system makes original authors and replicators antagonists. Focusing on the fields of economics, political science, sociology and psychology, in which ready access to raw data and software code are crucial to replication efforts, we survey deficiencies in the current system. We propose reforms that can both encourage and reinforce better behaviour — a system in which authors feel that replication of software code is both probable and fair, and in which less time and effort is required for replication. Current incentives for replication attempts reward those efforts that overturn the original results. In fact, in the 11 top-tier economics journals we surveyed, we could find only 11 replication studies — in this case, defined as reanalyses using the same data sets — published since 2011. All claimed to refute the original results. We also surveyed 88 editors and co-editors from these 11 journals. All editors who replied (35 in total, including at least one from each journal) said they would, in principle, publish a replication study that overturned the results of an original study. Only nine of the respondents said that they would consider publishing a replication study that confirmed the original results. We also personally experienced antagonism between replicators and authors in a programme sponsored by the International Initiative for Impact Evaluation (3ie), a non-governmental organization that actively funds software-code replication. We participated as authors of original studies (P.G. and S.G.) and as the chair of 3ie's board of directors (P.G.). In our experience, the programme worked liked this: 3ie selected influential papers to be replicated and then held an open competition, awarding approximately US{\$}25,000 for the replication of each study4. The organization also offered the original authors the opportunity to review and comment on the replications. Of 27 studies commissioned, 20 were completed, and 7 (35{\%}) reported that they were unable to fully replicate the results in the original article. The only replication published in a peer-reviewed journal5 claimed to refute the results of the original paper. Despite 3ie's best efforts, adversarial relationships developed between original and replication researchers. Original authors of five of the seven non-replicated studies wrote in public comments that the replications actively sought to refute their results and were nitpicking. One group stated that the incentives of replicators to publish “could lead to overstatement of the magnitude of criticism” (see go.nature.com/2gecz3b). Others made similar points. Although one effort replicated all the results in the original paper, the originating authors wrote, “we disagree with the unnecessarily aggressive tone of some statements in the replication report (particularly in the abstract)” (see go.nature.com/2esdjkr). Another group felt that “the statement that our original conclusions were robust was buried”(see go.nature.com/2siufet). The 3ie-sponsored replication5 of one highly cited paper6 resulted in an acrimonious debate that became known as the Worm Wars. Several independent scholars speculated that assumptions made by the replicators had more to do with overturning results than with any scientific justification. Replication costs A first step to getting more replications is making them easier. This could be done by requiring authors to publicly post the data and code used to produce the results in their studies — although this in itself would not redress the incentive to gain a publication by overturning results. Ready access to data and code would ease replication attempts. In our survey of journal websites, the mid-tier economics journals (as ranked by impact factor) and those in sociology and psychology rarely asked for these resources to be made available. By contrast, almost all of the top-tier journals in economics have policies that require software code and data to be made available to editors before publication. This is also true of most of the political-science journals we assessed, and all three of the broad science journals in our survey (see Supplementary information). In addition, many of the top-tier economics journals explicitly ask authors to post raw data as well as estimation data — the final data set used to produce the results after data clean-up and manipulation of variables. These are usually placed on a publicly accessible website that is designated or maintained by the journal. There seems to be no such norm in psychology and sociology journals (see ‘Data checked?'). ￼ Source: P. Gertler, S. Galiani {\&} M. Romero (unpublished data)},
author = {Gertler, Paul and Galiani, Sebastian and Romero, Mauricio},
doi = {10.1038/d41586-018-02108-9},
file = {:Users/rianaminocher/Documents/reading/Gertler{\_}et{\_}al{\_}2018{\_}Replication economics.pdf:pdf},
journal = {Nature},
pages = {417--419},
title = {{How to make replication the norm}},
url = {https://www.nature.com/articles/d41586-018-02108-9},
volume = {554},
year = {2018}
}
@article{manca2018meta,
author = {Manca, Andrea and Cugusi, Lucia and Dvir, Zeevi and Deriu, Franca},
doi = {10.1016/j.jclinepi.2018.01.009},
file = {:Users/rianaminocher/Documents/reading/Manca{\_}et{\_}al{\_}2018{\_}Non-correspondence meta-analysis.pdf:pdf},
issn = {1878-5921},
journal = {Journal of Clinical Epidemiology},
keywords = {Correspondence,Data extraction,Data sharing,Meta-analysis},
number = {0},
pmid = {29408344},
publisher = {Elsevier Inc},
title = {{Non-corresponding authors in the era of meta-analyses.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/29408344},
volume = {0},
year = {2018}
}
@article{patil2016statistical,
author = {Patil, Prasad and Peng, Roger D and Leek, Jeffrey T},
doi = {10.1101/066803},
file = {:Users/rianaminocher/Documents/reading/Patil{\_}2016{\_}Statistical definition for reproducibility and replicability.pdf:pdf},
journal = {BioRxiV},
pages = {8--13},
title = {{A statistical definition for reproducibility and replicability}},
year = {2016}
}
@article{ihle2017credible,
abstract = {Science is meant to be the systematic and objective study of the world but evidence suggests that scientific practices are sometimes falling short of this expectation. In this invited idea, we argue that any failure to conduct research according to a documented plan (lack of reliability) and/or any failure to ensure that reconducting the same project would provide the same finding (lack of reproducibility), will result in a low probability of independent studies reaching the same outcome (lack of replicability). After outlining the challenges facing behavioral ecology and science more broadly and incorporating advice from international organizations such as the Center for Open Science (COS), we present clear guidelines and tutorials on what we think open practices represent for behavioral ecologists. In addition, we indicate some of the currently most appropriate and freely available tools for adopting these practices. Finally, we suggest that all journals in our field, such as Behavioral Ecology, give additional weight to transparent studies and therefore provide greater incentives to align our scientific practices to our scientific values. Overall, we argue that producing demonstrably credible science is now fully achievable for the benefit of each researcher individually and for our community as a whole.},
author = {Ihle, Malika and Winney, Isabel S. and Krystalli, Anna and Croucher, Michael},
doi = {10.1093/beheco/arx003},
file = {:Users/rianaminocher/Documents/reading/Ihle{\_}2017{\_}Striving for transparent and credible research.pdf:pdf},
isbn = {1045-2249},
issn = {14657279},
journal = {Behavioral Ecology},
keywords = {Acknowledging Open Practices,Badges,Integrity,Open science initiative,Software,TOP guidelines,Toolkit},
number = {2},
pages = {348--354},
title = {{Striving for transparent and credible research: Practical guidelines for behavioral ecologists}},
volume = {28},
year = {2017}
}
@article{Wacker2017,
abstract = {Within multiple fields alarming reproducibility problems are now obvious to most: The majority of the reported effects are either false positives or the population effect size is much smaller than expected based on the initial studies (e.g., Ioannidis, 2005; Button et al., 2013; Open Science Collaboration, 2015; Baker, 2016; Nichols et al., 2017). Assuming that neither outright scientific fraud (Fanelli, 2009) nor severe deficits in methodological training are the norm, likely reasons for this inacceptable status quo include the following: (A) a high prevalence of severely underpowered studies (e.g., Button et al., 2013), (B) hypothesizing after results are known (HARKing; Kerr, 1998), (C) intentionally or unintentionally exploiting researcher degrees of freedom (Simmons et al., 2011) in data processing and analysis and thereby pushing the p-value of statistical tests below the conventional significance level without being transparent concerning all the variables and approaches that have been tried out (P-HACKING), and (D) selective reporting of research findings and publication bias. Several options for pre-registration of hypotheses are now readily available providing the opportunity to effectively prevent HARKing (e.g., OSF.io, AsPredicted.org). However, suggestions to address the other three issues have so far met with the following challenges:},
author = {Wacker, Jan},
doi = {10.3389/fpsyg.2017.01332},
file = {:Users/rianaminocher/Documents/reading/Wacker{\_}2017{\_}Forking path analysis.pdf:pdf},
issn = {16641078},
journal = {Frontiers in Psychology},
keywords = {Cooperative forking path analysis,False positives,Replication,Reproducibility,Researcher degrees of freedom,Scientific practice,Statistical power},
number = {AUG},
pages = {8--11},
title = {{Increasing the reproducibility of science through close cooperation and forking path analysis}},
volume = {8},
year = {2017}
}
@article{peng2011computational,
abstract = {Computational science has led to exciting new developments, but the nature of the work has exposed limitations in our ability to evaluate published findings. Reproducibility has the potential to serve as a minimum standard for judging scientific claims when full independent replication of a study is not possible.},
archivePrefix = {arXiv},
arxivId = {0901.4552},
author = {Peng, Roger D.},
doi = {10.1126/science.1213847},
eprint = {0901.4552},
file = {:Users/rianaminocher/Documents/reading/Peng{\_}2011{\_}Reproducible research.pdf:pdf},
isbn = {1095-9203 (Electronic)$\backslash$r0036-8075 (Linking)},
issn = {10959203},
journal = {Science},
number = {6060},
pages = {1226--1227},
pmid = {22144613},
title = {{Reproducible research in computational science}},
volume = {334},
year = {2011}
}
@article{teunis2015responsibility,
abstract = {BACKGROUND: Publication of a manuscript does not end an author's responsibilities. Reasons to contact an author after publication include clarification, access to raw data, and collaboration. However, legitimate questions have been raised regarding whether these responsibilities generally are being met by corresponding authors of biomedical publications.$\backslash$n$\backslash$nQUESTIONS/PURPOSES: This study aims to establish (1) what proportion of corresponding authors accept the responsibility of correspondence; (2) identify characteristics of responders; and (3) assess email address decay with time. We hypothesize that the response rate is unrelated to journal impact factor.$\backslash$n$\backslash$nMETHODS: We contacted 450 corresponding authors throughout various fields of biomedical research regarding the availability of additional data from their study, under the pretense of needing these data for a related review article. Authors were randomly selected from 45 journals whose impact factors ranged from 52 to 0; the source articles were published between May 2003 and May 2013. The proportion of corresponding authors who replied, along with author characteristics were recorded, as was the proportion of emails that were returned for inactive addresses; 446 authors were available for final analysis.$\backslash$n$\backslash$nRESULTS: Fifty-three percent (190/357) of the authors with working email addresses responded to our request. Clinical researchers were more likely to reply than basic/translational scientists (51{\%} [114/225] versus 34{\%} [76/221]; p{\textless}0.001). Impact factor and other author characteristics did not differ. Logistic regression analysis showed that the odds of replying decreased by 15{\%} per year (odds ratio [OR], 0.85; 95{\%} CI, 0.79-0.91; p{\textless}0.001), and showed a positive relationship between clinical research and response (OR, 2.0; 95{\%} CI, 1.3-2.9; p=0.001). In 2013 all email addresses (45/45) were reachable, but within 10 years, 49{\%} (21/43) had become invalid.$\backslash$n$\backslash$nCONCLUSIONS: Our results suggest that contacting corresponding authors is problematic throughout the field of biomedical research. Defining the responsibilities of corresponding authors by journals more explicitly-particularly after publication of their manuscript-may increase the response rate on data requests. Possible other ways to improve communication after research publication are: (1) listing more than one email address per corresponding author, eg, an institutional and personal address; (2) specifying all authors' email addresses; (3) when an author leaves an institution, send an automated reply offering alternative ways to get in touch; and (4) linking published manuscripts to research platforms.},
author = {Teunis, Teun and Nota, Sjoerd P F T and Schwab, Joseph H.},
doi = {10.1007/s11999-014-3868-3},
file = {:Users/rianaminocher/Documents/reading/Teunis{\_}et{\_}al{\_}2015{\_}Author responsibility.pdf:pdf},
issn = {15281132},
journal = {Clinical Orthopaedics and Related Research},
number = {2},
pages = {729--735},
pmid = {25123243},
title = {{Do Corresponding Authors Take Responsibility for Their Work? A Covert Survey}},
volume = {473},
year = {2015}
}
@article{savage2009sharing,
abstract = {BACKGROUND: Many journals now require authors share their data with other investigators, either by depositing the data in a public repository or making it freely available upon request. These policies are explicit, but remain largely untested. We sought to determine how well authors comply with such policies by requesting data from authors who had published in one of two journals with clear data sharing policies. METHODS AND FINDINGS: We requested data from ten investigators who had published in either PLoS Medicine or PLoS Clinical Trials. All responses were carefully documented. In the event that we were refused data, we reminded authors of the journal's data sharing guidelines. If we did not receive a response to our initial request, a second request was made. Following the ten requests for raw data, three investigators did not respond, four authors responded and refused to share their data, two email addresses were no longer valid, and one author requested further details. A reminder of PLoS's explicit requirement that authors share data did not change the reply from the four authors who initially refused. Only one author sent an original data set. CONCLUSIONS: We received only one of ten raw data sets requested. This suggests that journal policies requiring data sharing do not lead to authors making their data sets available to independent investigators.},
author = {Savage, Caroline J. and Vickers, Andrew J.},
doi = {10.1371/journal.pone.0007078},
file = {:Users/rianaminocher/Documents/reading/Savage{\_}2009{\_}Empirical.PDF:PDF},
isbn = {1932-6203},
issn = {19326203},
journal = {PLoS ONE},
number = {9},
pages = {9--11},
pmid = {19763261},
title = {{Empirical study of data sharing by authors publishing in PLoS journals}},
volume = {4},
year = {2009}
}
@article{gilbert2012structure,
abstract = {Reproducibility is the benchmark for results and conclusions drawn from scientific studies, but systematic studies on the reproducibility of scientific results are surprisingly rare. Moreover, many modern statistical methods make use of 'random walk' model fitting procedures, and these are inherently stochastic in their output. Does the combination of these statistical procedures and current standards of data archiving and method reporting permit the reproduction of the authors' results? To test this, we reanalysed data sets gathered from papers using the software package STRUCTURE to identify genetically similar clusters of individuals. We find that reproducing structure results can be difficult despite the straightforward requirements of the program. Our results indicate that 30{\%} of analyses were unable to reproduce the same number of population clusters. To improve this, we make recommendations for future use of the software and for reporting STRUCTURE analyses and results in published works. {\{}{\textcopyright}{\}} 2012 Blackwell Publishing Ltd.},
author = {Gilbert, Kimberly J. and Andrew, Rose L. and Bock, Dan G. and Franklin, Michelle T. and Kane, Nolan C. and Moore, Jean S{\'{e}}bastien and Moyers, Brook T. and Renaut, S{\'{e}}bastien and Rennison, Diana J. and Veen, Thor and Vines, Timothy H.},
doi = {10.1111/j.1365-294X.2012.05754.x},
file = {:Users/rianaminocher/Documents/reading/Gilbert{\_}et{\_}al{\_}2012{\_}STRUCTURE reproducibility.pdf:pdf},
isbn = {1365-294X},
issn = {09621083},
journal = {Molecular Ecology},
keywords = {population clustering,population genetics,reproducibility,structure},
number = {20},
pages = {4925--4930},
pmid = {22998190},
title = {{Recommendations for utilizing and reporting population genetic analyses: The reproducibility of genetic clustering using the program structure}},
volume = {21},
year = {2012}
}
@article{Roche2017,
author = {Roche, Dominique G},
doi = {10.1126/science.aan8158},
file = {:Users/rianaminocher/Documents/reading/Roche{\_}2017{\_}Science open data.pdf:pdf},
journal = {Science},
number = {6352},
pages = {654},
title = {{Evaluating Science's open-data policy}},
volume = {357},
year = {2017}
}
@article{andrew2015dfa,
abstract = {Data are the foundation of empirical research, yet all too often the datasets underlying published papers are unavailable, incorrect, or poorly curated. This is a serious issue, because future researchers are then unable to validate published results or reuse data to explore new ideas and hypotheses. Even if data files are securely stored and accessible, they must also be accompanied by accurate labels and identifiers. To assess how often problems with metadata or data curation affect the reproducibility of published results, we attempted to reproduce Discriminant Function Analyses (DFAs) from the field of organismal biology. DFA is a commonly used statistical analysis that has changed little since its inception almost eight decades ago, and therefore provides an opportunity to test reproducibility among datasets of varying ages. Out of 100 papers we initially surveyed, fourteen were excluded because they did not present the common types of quantitative result from their DFA or gave insufficient details of their DFA. Of the remaining 86 datasets, there were 15 cases for which we were unable to confidently relate the dataset we received to the one used in the published analysis. The reasons ranged from incomprehensible or absent variable labels, the DFA being performed on an unspecified subset of the data, or the dataset we received being incomplete. We focused on reproducing three common summary statistics from DFAs: the percent variance explained, the percentage correctly assigned and the largest discriminant function coefficient. The reproducibility of the first two was fairly high (20 of 26, and 44 of 60 datasets, respectively), whereas our success rate with the discriminant function coefficients was lower (15 of 26 datasets). When considering all three summary statistics, we were able to completely reproduce 46 (65{\%}) of 71 datasets. While our results show that a majority of studies are reproducible, they highlight the fact that many studies still are not the carefully curated research that the scientific community and public expects.},
author = {Andrew, Rose L. and Albert, Arianne Y.K. and Renaut, Sebastien and Rennison, Diana J. and Bock, Dan G. and Vines, Tim},
doi = {10.7717/peerj.1137},
file = {:Users/rianaminocher/Documents/reading/Andrew{\_}et{\_}al{\_}2015{\_}DFA reproducibility.pdf:pdf},
issn = {2167-8359},
journal = {PeerJ},
pages = {e1137},
title = {{Assessing the reproducibility of discriminant function analyses}},
url = {https://peerj.com/articles/1137},
volume = {3},
year = {2015}
}
@article{vines2014age,
abstract = {Policies ensuring that research data are available on public archives are increasingly being implemented at the government [1], funding agency [2-4], and journal [5, 6] level. These policies are predicated on the idea that authors are poor stewards of their data, particularly over the long term [7], and indeed many studies have found that authors are often unable or unwilling to share their data [8-11]. However, there are no systematic estimates of how the availability of research data changes with time since publication. We therefore requested data sets from a relatively homogenous set of 516 articles published between 2 and 22 years ago, and found that availability of the data was strongly affected by article age. For papers where the authors gave the status of their data, the odds of a data set being extant fell by 17{\%} per year. In addition, the odds that we could find a working e-mail address for the first, last, or corresponding author fell by 7{\%} per year. Our results reinforce the notion that, in the long term, research data cannot be reliably preserved by individual researchers, and further demonstrate the urgent need for policies mandating data sharing via public archives. {\textcopyright} 2014 Elsevier Ltd.},
annote = {possible step -
meta analyse the reproducibility literature: how does ev anth compare (in short) to other fields 'assessed' --{\textgreater} this may be an incentive for better practices / understanding where the issue lies},
archivePrefix = {arXiv},
arxivId = {1312.5670},
author = {Vines, Timothy H. and Albert, Arianne Y K and Andrew, Rose L. and D{\'{e}}barre, Florence and Bock, Dan G. and Franklin, Michelle T. and Gilbert, Kimberly J. and Moore, Jean S{\'{e}}bastien and Renaut, S{\'{e}}bastien and Rennison, Diana J.},
doi = {10.1016/j.cub.2013.11.014},
eprint = {1312.5670},
file = {:Users/rianaminocher/Documents/reading/Vines{\_} 2014{\_}Avaliability of research data declines rapidly with article age.pdf:pdf},
isbn = {0960-9822},
issn = {09609822},
journal = {Current Biology},
number = {1},
pages = {94--97},
pmid = {24361065},
title = {{The availability of research data declines rapidly with article age}},
volume = {24},
year = {2014}
}
@article{Culina2018,
abstract = {Open access to data is revolutionizing the sciences. To allow ecologists and evolutionary biologists to confidently find and use the existing data, we provide an overview of the landscape of online data infrastructures, and highlight the key points to consider when using open data. We introduce an online collaborative platform to keep a community-driven, updated list of the best sources that enable search for data in one interface. In doing so, our aim is to lower the barrier to accessing open data, and encourage its use by researchers hoping to increase the scope, reliability and value of their findings.},
author = {Culina, Antica and Baglioni, Miriam and Crowther, Tom W. and Visser, Marcel E. and Woutersen-Windhouwer, Saskia and Manghi, Paolo},
doi = {10.1038/s41559-017-0458-2},
file = {:Users/rianaminocher/Documents/reading/Culina{\_}2018{\_}Navigating unfolding open data landscape in eco evo.pdf:pdf},
issn = {2397-334X},
journal = {Nature Ecology {\&} Evolution},
number = {3},
pages = {420--426},
publisher = {Springer US},
title = {{Navigating the unfolding open data landscape in ecology and evolution}},
url = {http://www.nature.com/articles/s41559-017-0458-2},
volume = {2},
year = {2018}
}
